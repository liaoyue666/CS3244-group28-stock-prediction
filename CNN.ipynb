{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800b3b72-0e0e-46ce-a529-43d885e4241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences built: 27379 samples, lookback=10, features=22\n",
      "\n",
      "===== Training fold1 for 5-day high-vol regime (temporal CNN) =====\n",
      "Epoch 1/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.8131 - val_loss: 0.6972\n",
      "Epoch 2/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7020 - val_loss: 0.6467\n",
      "Epoch 3/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.6478 - val_loss: 0.6334\n",
      "Epoch 4/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.6167 - val_loss: 0.6084\n",
      "Epoch 5/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6109 - val_loss: 0.6035\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5964 - val_loss: 0.6063\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.5874 - val_loss: 0.5921\n",
      "Epoch 8/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.5815 - val_loss: 0.5875\n",
      "Epoch 9/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5738 - val_loss: 0.5837\n",
      "Epoch 10/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5683 - val_loss: 0.5809\n",
      "Epoch 11/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5586 - val_loss: 0.5774\n",
      "Epoch 12/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.5599 - val_loss: 0.5658\n",
      "Epoch 13/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.5618 - val_loss: 0.5621\n",
      "Epoch 14/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5446 - val_loss: 0.5665\n",
      "Epoch 15/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.5447 - val_loss: 0.5764\n",
      "Epoch 16/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.5402 - val_loss: 0.5655\n",
      "Epoch 17/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.5437 - val_loss: 0.5738\n",
      "Epoch 18/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.5388 - val_loss: 0.5615\n",
      "Epoch 19/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.5303 - val_loss: 0.5636\n",
      "Epoch 20/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.5291 - val_loss: 0.5794\n",
      "Epoch 21/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.5240 - val_loss: 0.5716\n",
      "Epoch 22/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.5217 - val_loss: 0.5733\n",
      "Epoch 23/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.5203 - val_loss: 0.5823\n",
      "Epoch 24/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.5140 - val_loss: 0.5885\n",
      "Epoch 25/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.5078 - val_loss: 0.5776\n",
      "Epoch 26/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.5102 - val_loss: 0.5864\n",
      "Epoch 27/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.5023 - val_loss: 0.5687\n",
      "Epoch 28/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4949 - val_loss: 0.5596\n",
      "Epoch 29/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.4901 - val_loss: 0.6189\n",
      "Epoch 30/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.4909 - val_loss: 0.6035\n",
      "Epoch 31/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.4814 - val_loss: 0.6099\n",
      "Epoch 32/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4874 - val_loss: 0.6335\n",
      "Epoch 33/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.4852 - val_loss: 0.5912\n",
      "Epoch 34/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4785 - val_loss: 0.6089\n",
      "Epoch 35/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4730 - val_loss: 0.5943\n",
      "Epoch 36/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.4708 - val_loss: 0.6390\n",
      "Epoch 37/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4612 - val_loss: 0.6417\n",
      "Epoch 38/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4558 - val_loss: 0.6083\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "===== Training fold2 for 5-day high-vol regime (temporal CNN) =====\n",
      "Epoch 1/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - loss: 0.8097 - val_loss: 0.6646\n",
      "Epoch 2/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.6733 - val_loss: 0.6765\n",
      "Epoch 3/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.6337 - val_loss: 0.6798\n",
      "Epoch 4/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6081 - val_loss: 0.6658\n",
      "Epoch 5/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6002 - val_loss: 0.6731\n",
      "Epoch 6/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5955 - val_loss: 0.6626\n",
      "Epoch 7/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5884 - val_loss: 0.6661\n",
      "Epoch 8/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.5735 - val_loss: 0.6636\n",
      "Epoch 9/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.5744 - val_loss: 0.6620\n",
      "Epoch 10/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.5717 - val_loss: 0.6702\n",
      "Epoch 11/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.5698 - val_loss: 0.6597\n",
      "Epoch 12/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.5598 - val_loss: 0.6636\n",
      "Epoch 13/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.5590 - val_loss: 0.6750\n",
      "Epoch 14/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.5507 - val_loss: 0.6747\n",
      "Epoch 15/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.5506 - val_loss: 0.6831\n",
      "Epoch 16/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5448 - val_loss: 0.6827\n",
      "Epoch 17/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.5436 - val_loss: 0.6815\n",
      "Epoch 18/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.5365 - val_loss: 0.6850\n",
      "Epoch 19/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.5351 - val_loss: 0.6896\n",
      "Epoch 20/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.5227 - val_loss: 0.6958\n",
      "Epoch 21/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5237 - val_loss: 0.6885\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "===== Training fold3 for 5-day high-vol regime (temporal CNN) =====\n",
      "Epoch 1/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - loss: 0.7917 - val_loss: 0.6473\n",
      "Epoch 2/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.6751 - val_loss: 0.6183\n",
      "Epoch 3/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6494 - val_loss: 0.6157\n",
      "Epoch 4/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6209 - val_loss: 0.6024\n",
      "Epoch 5/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.6101 - val_loss: 0.5973\n",
      "Epoch 6/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.6034 - val_loss: 0.5865\n",
      "Epoch 7/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.6007 - val_loss: 0.5852\n",
      "Epoch 8/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.5922 - val_loss: 0.5835\n",
      "Epoch 9/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.5895 - val_loss: 0.5803\n",
      "Epoch 10/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.5784 - val_loss: 0.5845\n",
      "Epoch 11/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.5740 - val_loss: 0.5840\n",
      "Epoch 12/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5729 - val_loss: 0.5687\n",
      "Epoch 13/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.5734 - val_loss: 0.5587\n",
      "Epoch 14/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.5653 - val_loss: 0.5788\n",
      "Epoch 15/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5622 - val_loss: 0.5729\n",
      "Epoch 16/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.5612 - val_loss: 0.5878\n",
      "Epoch 17/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.5549 - val_loss: 0.5806\n",
      "Epoch 18/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.5531 - val_loss: 0.5980\n",
      "Epoch 19/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5510 - val_loss: 0.5600\n",
      "Epoch 20/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.5482 - val_loss: 0.6004\n",
      "Epoch 21/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.5470 - val_loss: 0.6045\n",
      "Epoch 22/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.5398 - val_loss: 0.5810\n",
      "Epoch 23/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5290 - val_loss: 0.6129\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      " Per-fold test accuracy for 5-day high-vol regime (CNN)\n",
      "    fold  accuracy  baseline_acc  threshold  pos_ratio_train  n_train  n_test\n",
      "0  fold1  0.653439      0.423280       0.45              0.5     5884    1512\n",
      "1  fold2  0.741402      0.361772       0.50              0.5     7306    1512\n",
      "2  fold3  0.848270      0.235849       0.55              0.5     8818    1272\n",
      "\n",
      " Average test accuracy and baseline across folds\n",
      "CNN accuracy (mean): 0.7477039033642807\n",
      "Baseline accuracy (mean): 0.3403006555522279\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DATA_PATH = \"combined_cleaned.csv\"\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "\n",
    "# CNN hyperparameters\n",
    "HORIZON = 5          # future window for realized volatility\n",
    "LOOKBACK = 10        # past days in temporal window\n",
    "CHANNELS_1 = 64\n",
    "CHANNELS_2 = 64\n",
    "KERNEL_SIZE = 5\n",
    "HIDDEN_DIM = 64\n",
    "DROPOUT_CONV = 0.3\n",
    "DROPOUT_DENSE = 0.4\n",
    "HIGH_VOL_Q = 0.5\n",
    "\n",
    "# Base features\n",
    "BASE_FEATURES = [\"log_return\",\"lag1\",\"lag3\",\"lag5\",\"MA5\",\"MA20\",\"vol_5\",\"Volume\",\"rel_vol\",\"regime_high_vol\"]\n",
    "\n",
    "# Features inputted into the CNN\n",
    "CANDIDATE_FEATURES = [\"log_return\",\"lag1\",\"lag3\",\"lag5\",\"MA5\",\"MA20\",\"vol_5\",\"Volume\",\"rel_vol\",\"regime_high_vol\",# Base features\n",
    "    \"MA_diff\",\"return_over_vol\",\"volume_spike\",\"log_return_z_cs\",\"MA5_z_cs\",\"MA20_z_cs\",\"vol_5_z_cs\",\"Volume_z_cs\",\"rel_vol_z_cs\",\"log_return_rank_cs\",\"vol_5_rank_cs\",\"Volume_rank_cs\"] # Engineered features\n",
    "\n",
    "# Candidate probability thresholds for converting CNN outputs into binary labels (0/1)\n",
    "THRESHOLDS = [0.40, 0.45, 0.50, 0.55, 0.60]\n",
    "\n",
    "# Parse dates, sort by (Ticker, Date), and drop invalid / duplicate rows\n",
    "def load_and_basic_clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Date\"]).copy()\n",
    "    df = df.sort_values([\"Ticker\", \"Date\"])\n",
    "    df = df.drop_duplicates(subset=[\"Ticker\", \"Date\"])\n",
    "    return df\n",
    "\n",
    "# Compute log_return based on that ticker’s Close series if not already done so\n",
    "def ensure_log_return(df):\n",
    "    df = df.copy()\n",
    "    if \"log_return\" not in df.columns:\n",
    "        df[\"log_return\"] = (\n",
    "            np.log(df[\"Close\"]) - np.log(df.groupby(\"Ticker\")[\"Close\"].shift(1))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Add market regime classification based on the median of the market volatility\n",
    "def add_market_regime(df, vol_window=20, min_periods=10):\n",
    "    df = df.copy()\n",
    "    mkt_ret = (\n",
    "        df.groupby(\"Date\")[\"log_return\"]\n",
    "          .mean()\n",
    "          .sort_index()\n",
    "    )\n",
    "    mkt_vol = mkt_ret.rolling(window=vol_window, min_periods=min_periods).std()\n",
    "    vol_median = mkt_vol.median()\n",
    "    df[\"market_vol\"] = df[\"Date\"].map(mkt_vol)\n",
    "    df[\"regime_high_vol\"] = (df[\"market_vol\"] > vol_median).astype(int)\n",
    "    return df\n",
    "\n",
    "# Compute realized volatility for the next 5 days\n",
    "def add_realized_vol_5(df, horizon=5):\n",
    "    \"\"\"\n",
    "    realized_vol_5(t) = sqrt( (r_{t+1}^2 + ... + r_{t+horizon}^2) / horizon )\n",
    "    where r_t = log_return.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"realized_vol_5\"] = np.nan\n",
    "    for ticker, g in df.groupby(\"Ticker\"):\n",
    "        idx = g.index\n",
    "        r = g[\"log_return\"]\n",
    "        r_sq = r ** 2\n",
    "        acc = None\n",
    "        for i in range(1, horizon + 1):\n",
    "            shifted = r_sq.shift(-i)\n",
    "            acc = shifted if acc is None else acc + shifted\n",
    "        rv5 = np.sqrt(acc / horizon)\n",
    "        df.loc[idx, \"realized_vol_5\"] = rv5\n",
    "    return df\n",
    "\n",
    "# Add engineered features (MA_diff, return_over_vol, and volume_spike) that help the CNN see trend, risk-adjusted moves, and unusual volume.\n",
    "def add_interaction_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # MA_diff = MA5 - MA20\n",
    "    if \"MA5\" in df.columns and \"MA20\" in df.columns:\n",
    "        df[\"MA_diff\"] = df[\"MA5\"] - df[\"MA20\"]\n",
    "\n",
    "    # return_over_vol = log_return / (vol_5 + eps)\n",
    "    if \"log_return\" in df.columns and \"vol_5\" in df.columns:\n",
    "        eps = 1e-6\n",
    "        df[\"return_over_vol\"] = df[\"log_return\"] / (df[\"vol_5\"].replace(0, np.nan) + eps)\n",
    "\n",
    "    # volume_spike = Volume today / 20-day rolling mean Volume (per ticker, window ending today)\n",
    "    if \"Volume\" in df.columns:\n",
    "        df[\"volume_spike\"] = np.nan\n",
    "        for ticker, g in df.groupby(\"Ticker\"):\n",
    "            idx = g.index\n",
    "            vol = g[\"Volume\"]\n",
    "            vol_ma20 = vol.rolling(window=20, min_periods=5).mean()  # past 20 days\n",
    "            df.loc[idx, \"volume_spike\"] = vol / (vol_ma20.replace(0, np.nan))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add cross-sectional z-scores and ranks to help the CNN see how each feature compares to others on the same day.\n",
    "def add_cross_sectional_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    cs_cols_for_z = [\"log_return\",\"MA5\",\"MA20\",\"vol_5\",\"Volume\",\"rel_vol\"]\n",
    "\n",
    "    # Add z-scores for each feature\n",
    "    for col in cs_cols_for_z:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        group = df.groupby(\"Date\")[col]\n",
    "        mean = group.transform(\"mean\")\n",
    "        std = group.transform(\"std\").replace(0, np.nan)\n",
    "        z_name = f\"{col}_z_cs\"\n",
    "        df[z_name] = (df[col] - mean) / std\n",
    "        df[z_name] = df[z_name].fillna(0.0)  # default to 0 if no std\n",
    "\n",
    "    # Add rank-based features (0-1, within each date)\n",
    "    rank_cols = [\"log_return\", \"vol_5\", \"Volume\"]\n",
    "    for col in rank_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        r_name = f\"{col}_rank_cs\"\n",
    "        df[r_name] = (\n",
    "            df.groupby(\"Date\")[col]\n",
    "              .rank(pct=True)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Get the features that are actually in the dataframe\n",
    "def get_feature_columns(df):\n",
    "    feature_cols = [c for c in CANDIDATE_FEATURES if c in df.columns]\n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"No candidate features found in dataframe.\")\n",
    "    df = df.dropna(subset=feature_cols).copy()\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    return df, feature_cols\n",
    "\n",
    "# Define the folds for the cross-validation\n",
    "def define_folds():\n",
    "    folds = [\n",
    "        {\n",
    "            \"name\": \"fold1\",\n",
    "            \"train_start\": \"2009-01-01\",\n",
    "            \"train_end\":   \"2013-12-31\",\n",
    "            \"val_start\":   \"2014-01-01\",\n",
    "            \"val_end\":     \"2014-12-31\",\n",
    "            \"test_start\":  \"2015-01-01\",\n",
    "            \"test_end\":    \"2015-12-31\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"fold2\",\n",
    "            \"train_start\": \"2009-01-01\",\n",
    "            \"train_end\":   \"2014-12-31\",\n",
    "            \"val_start\":   \"2015-01-01\",\n",
    "            \"val_end\":     \"2015-12-31\",\n",
    "            \"test_start\":  \"2016-01-01\",\n",
    "            \"test_end\":    \"2016-12-31\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"fold3\",\n",
    "            \"train_start\": \"2009-01-01\",\n",
    "            \"train_end\":   \"2015-12-31\",\n",
    "            \"val_start\":   \"2016-01-01\",\n",
    "            \"val_end\":     \"2016-12-31\",\n",
    "            \"test_start\":  \"2017-01-01\",\n",
    "            \"test_end\":    \"2017-12-31\",\n",
    "        },\n",
    "    ]\n",
    "    for f in folds:\n",
    "        for k in [\"train_start\",\"train_end\",\"val_start\",\"val_end\",\"test_start\",\"test_end\"]:\n",
    "            f[k] = pd.to_datetime(f[k])\n",
    "    return folds\n",
    "\n",
    "# Build sequences: X (N, LOOKBACK, d), rv5 (N,), dates (N,) (N is the number of samples, L is the lookback window, d is the number of features)\n",
    "def build_sequences(df, feature_cols, lookback):\n",
    "    X_list, rv_list, date_list, ticker_list = [], [], [], []\n",
    "    for ticker, g in df.groupby(\"Ticker\"):\n",
    "        g = g.sort_values(\"Date\")\n",
    "        feats = g[feature_cols].values\n",
    "        rv5 = g[\"realized_vol_5\"].values\n",
    "        dates = g[\"Date\"].values\n",
    "        tickers = g[\"Ticker\"].values\n",
    "\n",
    "        if len(g) < lookback:\n",
    "            continue\n",
    "\n",
    "        for i in range(lookback - 1, len(g)):\n",
    "            if np.isnan(rv5[i]):\n",
    "                continue  # need full future 5 days\n",
    "            X_list.append(feats[i - lookback + 1:i + 1, :])\n",
    "            rv_list.append(rv5[i])\n",
    "            date_list.append(dates[i])\n",
    "            ticker_list.append(tickers[i])\n",
    "\n",
    "    X = np.array(X_list)          # (N, L, d)\n",
    "    rv = np.array(rv_list)        # (N,)\n",
    "    dates = np.array(date_list)   # (N,)\n",
    "    tickers = np.array(ticker_list)\n",
    "    return X, rv, dates, tickers\n",
    "\n",
    "\n",
    "# Build the temporal CNN model\n",
    "def build_temporal_cnn_single(lookback, n_features):\n",
    "    inp = tf.keras.Input(shape=(lookback, n_features))\n",
    "    x = tf.keras.layers.Conv1D(filters=CHANNELS_1, kernel_size=KERNEL_SIZE, activation=\"relu\", padding=\"same\")(inp)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT_CONV)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=CHANNELS_2, kernel_size=KERNEL_SIZE, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT_CONV)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(HIDDEN_DIM, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT_DENSE)(x)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "        loss=\"binary_crossentropy\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Tune the threshold for the binary classification\n",
    "def tune_threshold(y_val, p_val, thresholds):\n",
    "    best_tau = 0.5\n",
    "    best_acc = -1.0\n",
    "    for tau in thresholds:\n",
    "        preds = (p_val >= tau).astype(int)\n",
    "        acc = accuracy_score(y_val, preds)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_tau = tau\n",
    "    return best_tau\n",
    "\n",
    "# Train one fold of the high-vol regime\n",
    "def train_one_fold_highvol(X, rv5, dates, feature_cols, fold):\n",
    "    train_mask = (dates >= fold[\"train_start\"]) & (dates <= fold[\"train_end\"])\n",
    "    val_mask   = (dates >= fold[\"val_start\"])   & (dates <= fold[\"val_end\"])\n",
    "    test_mask  = (dates >= fold[\"test_start\"])  & (dates <= fold[\"test_end\"])\n",
    "\n",
    "    X_train, rv_train = X[train_mask], rv5[train_mask]\n",
    "    X_val,   rv_val   = X[val_mask],   rv5[val_mask]\n",
    "    X_test,  rv_test  = X[test_mask],  rv5[test_mask]\n",
    "\n",
    "    if X_train.shape[0] == 0 or X_val.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "        print(f\"[{fold['name']}] skipped (empty split)\")\n",
    "        return None\n",
    "\n",
    "    # Define high-vol threshold from train realized_vol_5 only (no leakage) (quantile is used to find the threshold for the binary classification)\n",
    "    thr = np.quantile(rv_train, HIGH_VOL_Q)\n",
    "    y_train = (rv_train >= thr).astype(int)\n",
    "    y_val   = (rv_val   >= thr).astype(int)\n",
    "    y_test  = (rv_test  >= thr).astype(int)\n",
    "\n",
    "    # Baseline: always predict the majority class as defined on train, and compute its accuracy on the test set.\n",
    "    pos_ratio_train = y_train.mean()\n",
    "    majority_label = 1 if pos_ratio_train >= 0.5 else 0\n",
    "    baseline_acc_test = (y_test == majority_label).mean()\n",
    "\n",
    "    # Scale features per fold (standardization is used to normalize the features)\n",
    "    N_train, L, d = X_train.shape\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(N_train * L, d)\n",
    "    scaler.fit(X_train_flat)\n",
    "\n",
    "    def transform(X_in):\n",
    "        N, L, d = X_in.shape\n",
    "        X_flat = X_in.reshape(N * L, d)\n",
    "        X_scaled = scaler.transform(X_flat)\n",
    "        return X_scaled.reshape(N, L, d)\n",
    "\n",
    "    X_train_scaled = transform(X_train)\n",
    "    X_val_scaled   = transform(X_val)\n",
    "    X_test_scaled  = transform(X_test)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_train_scaled, y_train.astype(\"float32\"))\n",
    "    ).shuffle(buffer_size=X_train_scaled.shape[0]).batch(BATCH_SIZE)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_val_scaled, y_val.astype(\"float32\"))\n",
    "    ).batch(BATCH_SIZE)\n",
    "\n",
    "    model = build_temporal_cnn_single(LOOKBACK, len(feature_cols))\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    print(f\"\\n===== Training {fold['name']} for 5-day high-vol regime (temporal CNN) =====\")\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[es], verbose=1)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val_scaled, y_val.astype(\"float32\"))).batch(BATCH_SIZE)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test.astype(\"float32\"))).batch(BATCH_SIZE)\n",
    "\n",
    "    p_val = model.predict(val_ds).ravel()\n",
    "    p_test = model.predict(test_ds).ravel()\n",
    "\n",
    "    best_tau = tune_threshold(y_val, p_val, THRESHOLDS)\n",
    "\n",
    "    y_pred_test = (p_test >= best_tau).astype(int)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    result = {\n",
    "        \"fold\": fold[\"name\"],\n",
    "        \"accuracy\": acc_test,\n",
    "        \"baseline_acc\": baseline_acc_test,\n",
    "        \"threshold\": best_tau,\n",
    "        \"pos_ratio_train\": float(pos_ratio_train),\n",
    "        \"n_train\": int(X_train.shape[0]),\n",
    "        \"n_test\": int(X_test.shape[0]),\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Pipeline for CNN\n",
    "def run_highvol_cnn():\n",
    "    df = load_and_basic_clean(DATA_PATH)\n",
    "    df = ensure_log_return(df)\n",
    "    df = add_market_regime(df)\n",
    "    df = add_realized_vol_5(df, horizon=HORIZON)\n",
    "\n",
    "    # Add engineered features\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_cross_sectional_features(df)\n",
    "\n",
    "    # Drop rows without realized_vol_5, which is needed to define the high-vol / low-vol classification label.\n",
    "    df = df.dropna(subset=[\"realized_vol_5\"]).copy()\n",
    "\n",
    "    df, feature_cols = get_feature_columns(df)\n",
    "\n",
    "    X, rv5, dates, tickers = build_sequences(df, feature_cols, LOOKBACK)\n",
    "    print(f\"Sequences built: {X.shape[0]} samples, lookback={LOOKBACK}, features={X.shape[2]}\")\n",
    "\n",
    "    folds = define_folds()\n",
    "    all_results = []\n",
    "\n",
    "    for fold in folds:\n",
    "        res = train_one_fold_highvol(X, rv5, dates, feature_cols, fold)\n",
    "        if res is not None:\n",
    "            all_results.append(res)\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    print(\"\\n Per-fold test accuracy for 5-day high-vol regime (CNN)\")\n",
    "    print(results_df[[\"fold\", \"accuracy\", \"baseline_acc\", \"threshold\", \"pos_ratio_train\", \"n_train\", \"n_test\"]])\n",
    "    print(\"\\n Average test accuracy and baseline across folds\")\n",
    "    print(\"CNN accuracy (mean):\", results_df[\"accuracy\"].mean())\n",
    "    print(\"Baseline accuracy (mean):\", results_df[\"baseline_acc\"].mean())\n",
    "\n",
    "run_highvol_cnn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98211d5-3f9b-4a1d-9b44-b09d1a00a326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
